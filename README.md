# ðŸ“˜ Machine Learning Interview Prep Notebooks

This repository contains a structured set of Jupyter Notebooks designed to build **mathematical foundations** and **core ML understanding** step by step. Each notebook introduces concepts through theory, formulas, and hands-on Python code.

---

## âœ… Contents

### **NB-1: Math & Stats Fundamentals**
Covers the essential mathematical and statistical concepts behind ML:

- **Q1:** L1 vs L2 norms â€“ definitions, differences, and geometric intuition  
- **Q2:** Dot product â€“ similarity, projections, and angles between vectors  
- **Q3:** Variance & covariance â€“ spread of data and relationships between variables  
- **Q4:** Bayesâ€™ rule â€“ updating beliefs with new evidence (spam filtering example)  
- **Q5:** Law of Large Numbers (LLN) & Central Limit Theorem (CLT) â€“ why sampling and normal approximations work  
- **Q6:** Biasâ€“variance tradeoff â€“ decomposition of error, underfitting vs overfitting  

---

### **NB-2: Machine Learning Fundamentals**
Introduces and demonstrates core ML algorithms and evaluation methods:

- **Q1:** Supervised, Unsupervised, Reinforcement Learning â€“ definitions & examples (k-NN, clustering demo)  
- **Q2:** Overfitting vs underfitting â€“ polynomial regression visualization  
- **Q3:** Regression algorithms â€“ Linear, Polynomial, Ridge (L2), Lasso (L1); coefficients and curves  
- **Q4:** Classification algorithms â€“ k-NN, Decision Trees, Naive Bayes, SVM  
  - k-NN â†’ queries & neighbors  
  - Decision Trees â†’ boundaries + tree diagram + feature importances  
  - Naive Bayes â†’ Gaussian likelihood contours (Ïƒ ellipses)  
  - SVM â†’ hyperplanes, margins, support vectors  
- **Q5:** Cross-validation â€“ k-fold, stratified, leave-one-out  
- **Q6:** Biasâ€“variance in ML â€“ training vs test error tradeoff  
- **Q7:** Confusion matrix & metrics â€“ accuracy, precision, recall, F1  
  - Real-life scenarios where **precision** or **recall** matters  
  - Easy **memory trick** for formulas  
- **Q8:** ROC & AUC â€“ with best (AUC â‰ˆ 1.0), random (AUC â‰ˆ 0.5), and worst (AUC < 0.5) examples  

---

## ðŸš€ Upcoming: **NB-3**
The third notebook will cover **advanced ML topics**, such as:
- Feature engineering & preprocessing  
- Ensemble methods (Bagging, Boosting, Random Forests)  
- Gradient descent & optimization basics  
- Introduction to neural networks  

---

## ðŸ’¡ How to Use
1. Open notebooks in Jupyter or VS Code.  
2. Read the **theory sections** in Markdown.  
3. Run the **Python examples** to see concepts in action.  
4. Experiment: tweak parameters (e.g., `k` in k-NN, polynomial degree, regularization strength).  

---

## ðŸ“‚ Repository Structure
```
â”œâ”€â”€ 1_math_stats_fundamentals.ipynb   # NB-1: Math & Stats
â”œâ”€â”€ 2_ml_fundamentals.ipynb           # NB-2: Core ML
â”œâ”€â”€ 3_ml_advanced.ipynb (planned)     # NB-3: Advanced ML
â””â”€â”€ README.md                         # Project overview
```

---

âœ¨ With these notebooks, youâ€™ll build a **solid foundation** for ML interviews â€” progressing from math to algorithms to evaluation.  
